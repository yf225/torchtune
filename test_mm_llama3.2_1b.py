import torch

"""
 - aten.mm                                             31.823T     98.48%
                                                        1.473T      4.56%  ['(1372, 2048)(2048, 1):torch.bfloat16', '(2048, 8192)(1, 2048):torch.bfloat16']
                                                        1.473T      4.56%  ['(8192, 1372)(1, 8192):torch.bfloat16', '(1372, 2048)(2048, 1):torch.bfloat16']
                                                        1.473T      4.56%  ['(1372, 8192)(8192, 1):torch.bfloat16', '(8192, 2048)(2048, 1):torch.bfloat16']
                                                        1.246T      3.85%  ['(1160, 2048)(2048, 1):torch.bfloat16', '(2048, 8192)(1, 2048):torch.bfloat16']
                                                        1.246T      3.85%  ['(8192, 1160)(1, 8192):torch.bfloat16', '(1160, 2048)(2048, 1):torch.bfloat16']
                                                        1.246T      3.85%  ['(1160, 8192)(8192, 1):torch.bfloat16', '(8192, 2048)(2048, 1):torch.bfloat16']
                                                        0.737T      2.28%  ['(1372, 8192)(8192, 1):torch.bfloat16', '(8192, 2048)(1, 8192):torch.bfloat16']
                                                        0.737T      2.28%  ['(2048, 1372)(1, 2048):torch.bfloat16', '(1372, 8192)(8192, 1):torch.bfloat16']
                                                        0.737T      2.28%  ['(1372, 2048)(2048, 1):torch.bfloat16', '(2048, 8192)(8192, 1):torch.bfloat16']
                                                        0.696T      2.15%  ['(648, 2048)(2048, 1):torch.bfloat16', '(2048, 8192)(1, 2048):torch.bfloat16']
                                                        0.696T      2.15%  ['(8192, 648)(1, 8192):torch.bfloat16', '(648, 2048)(2048, 1):torch.bfloat16']
                                                        0.696T      2.15%  ['(648, 8192)(8192, 1):torch.bfloat16', '(8192, 2048)(2048, 1):torch.bfloat16']
                                                        0.674T      2.09%  ['(628, 2048)(2048, 1):torch.bfloat16', '(2048, 8192)(1, 2048):torch.bfloat16']
                                                        0.674T      2.09%  ['(8192, 628)(1, 8192):torch.bfloat16', '(628, 2048)(2048, 1):torch.bfloat16']
                                                        0.674T      2.09%  ['(628, 8192)(8192, 1):torch.bfloat16', '(8192, 2048)(2048, 1):torch.bfloat16']
                                                        0.633T      1.96%  ['(172, 2048)(2048, 1):torch.bfloat16', '(2048, 128256)(1, 2048):torch.bfloat16']
                                                        0.633T      1.96%  ['(128256, 172)(1, 128256):torch.bfloat16', '(172, 2048)(2048, 1):torch.bfloat16']
                                                        0.633T      1.96%  ['(172, 128256)(128256, 1):torch.bfloat16', '(128256, 2048)(2048, 1):torch.bfloat16']
                                                        0.623T      1.93%  ['(2048, 1160)(1, 2048):torch.bfloat16', '(1160, 8192)(8192, 1):torch.bfloat16']
                                                        0.623T      1.93%  ['(1160, 2048)(2048, 1):torch.bfloat16', '(2048, 8192)(8192, 1):torch.bfloat16']
                                                        0.544T      1.68%  ['(148, 2048)(2048, 1):torch.bfloat16', '(2048, 128256)(1, 2048):torch.bfloat16']
                                                        0.544T      1.68%  ['(128256, 148)(1, 128256):torch.bfloat16', '(148, 2048)(2048, 1):torch.bfloat16']
                                                        0.544T      1.68%  ['(148, 128256)(128256, 1):torch.bfloat16', '(128256, 2048)(2048, 1):torch.bfloat16']
                                                        0.520T      1.61%  ['(8192, 484)(1, 8192):torch.bfloat16', '(484, 2048)(2048, 1):torch.bfloat16']
                                                        0.520T      1.61%  ['(484, 8192)(8192, 1):torch.bfloat16', '(8192, 2048)(2048, 1):torch.bfloat16']
                                                        0.368T      1.14%  ['(2048, 1372)(1, 2048):torch.bfloat16', '(1372, 2048)(2048, 1):torch.bfloat16']
                                                        0.368T      1.14%  ['(1372, 2048)(2048, 1):torch.bfloat16', '(2048, 2048)(2048, 1):torch.bfloat16']
                                                        0.348T      1.08%  ['(2048, 648)(1, 2048):torch.bfloat16', '(648, 8192)(8192, 1):torch.bfloat16']
                                                        0.348T      1.08%  ['(648, 2048)(2048, 1):torch.bfloat16', '(2048, 8192)(8192, 1):torch.bfloat16']
                                                        0.337T      1.04%  ['(2048, 628)(1, 2048):torch.bfloat16', '(628, 8192)(8192, 1):torch.bfloat16']
                                                        0.337T      1.04%  ['(628, 2048)(2048, 1):torch.bfloat16', '(2048, 8192)(8192, 1):torch.bfloat16']
                                                        0.311T      0.96%  ['(2048, 1160)(1, 2048):torch.bfloat16', '(1160, 2048)(2048, 1):torch.bfloat16']
                                                        0.311T      0.96%  ['(1160, 2048)(2048, 1):torch.bfloat16', '(2048, 2048)(2048, 1):torch.bfloat16']
                                                        0.309T      0.96%  ['(84, 2048)(2048, 1):torch.bfloat16', '(2048, 128256)(1, 2048):torch.bfloat16']
                                                        0.309T      0.96%  ['(128256, 84)(1, 128256):torch.bfloat16', '(84, 2048)(2048, 1):torch.bfloat16']
                                                        0.309T      0.96%  ['(84, 128256)(128256, 1):torch.bfloat16', '(128256, 2048)(2048, 1):torch.bfloat16']
                                                        0.294T      0.91%  ['(80, 2048)(2048, 1):torch.bfloat16', '(2048, 128256)(1, 2048):torch.bfloat16']
                                                        0.294T      0.91%  ['(128256, 80)(1, 128256):torch.bfloat16', '(80, 2048)(2048, 1):torch.bfloat16']
                                                        0.294T      0.91%  ['(80, 128256)(128256, 1):torch.bfloat16', '(128256, 2048)(2048, 1):torch.bfloat16']
                                                        0.260T      0.80%  ['(2048, 484)(1, 2048):torch.bfloat16', '(484, 8192)(8192, 1):torch.bfloat16']
                                                        0.260T      0.80%  ['(484, 2048)(2048, 1):torch.bfloat16', '(2048, 8192)(8192, 1):torch.bfloat16']
                                                        0.235T      0.73%  ['(64, 2048)(2048, 1):torch.bfloat16', '(2048, 128256)(1, 2048):torch.bfloat16']
                                                        0.235T      0.73%  ['(128256, 64)(1, 128256):torch.bfloat16', '(64, 2048)(2048, 1):torch.bfloat16']
                                                        0.235T      0.73%  ['(64, 128256)(128256, 1):torch.bfloat16', '(128256, 2048)(2048, 1):torch.bfloat16']
                                                        0.174T      0.54%  ['(2048, 648)(1, 2048):torch.bfloat16', '(648, 2048)(2048, 1):torch.bfloat16']
                                                        0.174T      0.54%  ['(648, 2048)(2048, 1):torch.bfloat16', '(2048, 2048)(2048, 1):torch.bfloat16']
                                                        0.169T      0.52%  ['(2048, 628)(1, 2048):torch.bfloat16', '(628, 2048)(2048, 1):torch.bfloat16']
                                                        0.169T      0.52%  ['(628, 2048)(2048, 1):torch.bfloat16', '(2048, 2048)(2048, 1):torch.bfloat16']
                                                        0.130T      0.40%  ['(2048, 484)(1, 2048):torch.bfloat16', '(484, 2048)(2048, 1):torch.bfloat16']
                                                        0.130T      0.40%  ['(484, 2048)(2048, 1):torch.bfloat16', '(2048, 2048)(2048, 1):torch.bfloat16']
                                                        0.092T      0.28%  ['(512, 1372)(1, 512):torch.bfloat16', '(1372, 2048)(2048, 1):torch.bfloat16']
                                                        0.092T      0.28%  ['(1372, 512)(512, 1):torch.bfloat16', '(512, 2048)(2048, 1):torch.bfloat16']
                                                        0.088T      0.27%  ['(168, 2048)(2048, 1):torch.bfloat16', '(2048, 128256)(1, 2048):torch.bfloat16']
                                                        0.088T      0.27%  ['(128256, 168)(1, 128256):torch.bfloat16', '(168, 2048)(2048, 1):torch.bfloat16']
                                                        0.088T      0.27%  ['(168, 128256)(128256, 1):torch.bfloat16', '(128256, 2048)(2048, 1):torch.bfloat16']
                                                        0.078T      0.24%  ['(512, 1160)(1, 512):torch.bfloat16', '(1160, 2048)(2048, 1):torch.bfloat16']
                                                        0.078T      0.24%  ['(1160, 512)(512, 1):torch.bfloat16', '(512, 2048)(2048, 1):torch.bfloat16']
                                                        0.065T      0.20%  ['(124, 2048)(2048, 1):torch.bfloat16', '(2048, 128256)(1, 2048):torch.bfloat16']
                                                        0.065T      0.20%  ['(128256, 124)(1, 128256):torch.bfloat16', '(124, 2048)(2048, 1):torch.bfloat16']
                                                        0.065T      0.20%  ['(124, 128256)(128256, 1):torch.bfloat16', '(128256, 2048)(2048, 1):torch.bfloat16']
                                                        0.043T      0.13%  ['(512, 648)(1, 512):torch.bfloat16', '(648, 2048)(2048, 1):torch.bfloat16']
                                                        0.043T      0.13%  ['(648, 512)(512, 1):torch.bfloat16', '(512, 2048)(2048, 1):torch.bfloat16']
                                                        0.042T      0.13%  ['(512, 628)(1, 512):torch.bfloat16', '(628, 2048)(2048, 1):torch.bfloat16']
                                                        0.042T      0.13%  ['(628, 512)(512, 1):torch.bfloat16', '(512, 2048)(2048, 1):torch.bfloat16']
                                                        0.036T      0.11%  ['(68, 2048)(2048, 1):torch.bfloat16', '(2048, 128256)(1, 2048):torch.bfloat16']
                                                        0.036T      0.11%  ['(128256, 68)(1, 128256):torch.bfloat16', '(68, 2048)(2048, 1):torch.bfloat16']
                                                        0.036T      0.11%  ['(68, 128256)(128256, 1):torch.bfloat16', '(128256, 2048)(2048, 1):torch.bfloat16']
                                                        0.032T      0.10%  ['(512, 484)(1, 512):torch.bfloat16', '(484, 2048)(2048, 1):torch.bfloat16']
                                                        0.032T      0.10%  ['(484, 512)(512, 1):torch.bfloat16', '(512, 2048)(2048, 1):torch.bfloat16']
                                                        0.032T      0.10%  ['(60, 2048)(2048, 1):torch.bfloat16', '(2048, 128256)(1, 2048):torch.bfloat16']
                                                        0.032T      0.10%  ['(128256, 60)(1, 128256):torch.bfloat16', '(60, 2048)(2048, 1):torch.bfloat16']
                                                        0.032T      0.10%  ['(60, 128256)(128256, 1):torch.bfloat16', '(128256, 2048)(2048, 1):torch.bfloat16']
                                                        0.019T      0.06%  ['(36, 2048)(2048, 1):torch.bfloat16', '(2048, 128256)(1, 2048):torch.bfloat16']
                                                        0.019T      0.06%  ['(128256, 36)(1, 128256):torch.bfloat16', '(36, 2048)(2048, 1):torch.bfloat16']
                                                        0.019T      0.06%  ['(36, 128256)(128256, 1):torch.bfloat16', '(128256, 2048)(2048, 1):torch.bfloat16']
"""

def create_tensor_with_strides(shape, stride, dtype, device):
    # Create tensor with specific shape and strides
    tensor = torch.randn(*shape, dtype=dtype, device=device)
    tensor.as_strided(shape, stride)
    return tensor

def run_mm_operations():
    assert torch.cuda.is_available()
    device = torch.device("cuda")
    dtype = torch.bfloat16
    
    # Each entry is a pair of operations: (shape1, stride1, shape2, stride2)
    shapes = [
        # 4.56%
        ((1372, 2048), (2048, 1), (2048, 8192), (1, 2048)),
        ((8192, 1372), (1, 8192), (1372, 2048), (2048, 1)),
        ((1372, 8192), (8192, 1), (8192, 2048), (2048, 1)),
        
        # 3.85%
        ((1160, 2048), (2048, 1), (2048, 8192), (1, 2048)),
        ((8192, 1160), (1, 8192), (1160, 2048), (2048, 1)),
        ((1160, 8192), (8192, 1), (8192, 2048), (2048, 1)),
        
        # 2.28%
        ((1372, 8192), (8192, 1), (8192, 2048), (1, 8192)),
        ((2048, 1372), (1, 2048), (1372, 8192), (8192, 1)),
        ((1372, 2048), (2048, 1), (2048, 8192), (8192, 1)),
        
        # 2.15%
        ((648, 2048), (2048, 1), (2048, 8192), (1, 2048)),
        ((8192, 648), (1, 8192), (648, 2048), (2048, 1)),
        ((648, 8192), (8192, 1), (8192, 2048), (2048, 1)),
        
        # 2.09%
        ((628, 2048), (2048, 1), (2048, 8192), (1, 2048)),
        ((8192, 628), (1, 8192), (628, 2048), (2048, 1)),
        ((628, 8192), (8192, 1), (8192, 2048), (2048, 1)),
        
        # 1.96%
        ((172, 2048), (2048, 1), (2048, 128256), (1, 2048)),
        ((128256, 172), (1, 128256), (172, 2048), (2048, 1)),
        ((172, 128256), (128256, 1), (128256, 2048), (2048, 1)),
        
        # 1.93%
        ((2048, 1160), (1, 2048), (1160, 8192), (8192, 1)),
        ((1160, 2048), (2048, 1), (2048, 8192), (8192, 1)),
        
        # 1.68%
        ((148, 2048), (2048, 1), (2048, 128256), (1, 2048)),
        ((128256, 148), (1, 128256), (148, 2048), (2048, 1)),
        ((148, 128256), (128256, 1), (128256, 2048), (2048, 1)),
        
        # 1.61%
        ((8192, 484), (1, 8192), (484, 2048), (2048, 1)),
        ((484, 8192), (8192, 1), (8192, 2048), (2048, 1)),
        
        # 1.14%
        ((2048, 1372), (1, 2048), (1372, 2048), (2048, 1)),
        ((1372, 2048), (2048, 1), (2048, 2048), (2048, 1)),
        
        # 1.08%
        ((2048, 648), (1, 2048), (648, 8192), (8192, 1)),
        ((648, 2048), (2048, 1), (2048, 8192), (8192, 1)),
        
        # 1.04%
        ((2048, 628), (1, 2048), (628, 8192), (8192, 1)),
        ((628, 2048), (2048, 1), (2048, 8192), (8192, 1)),
        
        # 0.96%
        ((2048, 1160), (1, 2048), (1160, 2048), (2048, 1)),
        ((1160, 2048), (2048, 1), (2048, 2048), (2048, 1)),
        ((84, 2048), (2048, 1), (2048, 128256), (1, 2048)),
        ((128256, 84), (1, 128256), (84, 2048), (2048, 1)),
        ((84, 128256), (128256, 1), (128256, 2048), (2048, 1)),
        
        # 0.91%
        ((80, 2048), (2048, 1), (2048, 128256), (1, 2048)),
        ((128256, 80), (1, 128256), (80, 2048), (2048, 1)),
        ((80, 128256), (128256, 1), (128256, 2048), (2048, 1)),
        
        # 0.80%
        ((2048, 484), (1, 2048), (484, 8192), (8192, 1)),
        ((484, 2048), (2048, 1), (2048, 8192), (8192, 1)),
        
        # 0.73%
        ((64, 2048), (2048, 1), (2048, 128256), (1, 2048)),
        ((128256, 64), (1, 128256), (64, 2048), (2048, 1)),
        ((64, 128256), (128256, 1), (128256, 2048), (2048, 1)),
        
        # 0.54%
        ((2048, 648), (1, 2048), (648, 2048), (2048, 1)),
        ((648, 2048), (2048, 1), (2048, 2048), (2048, 1)),
        
        # 0.52%
        ((2048, 628), (1, 2048), (628, 2048), (2048, 1)),
        ((628, 2048), (2048, 1), (2048, 2048), (2048, 1)),
        
        # 0.40%
        ((2048, 484), (1, 2048), (484, 2048), (2048, 1)),
        ((484, 2048), (2048, 1), (2048, 2048), (2048, 1)),
        
        # 0.28%
        ((512, 1372), (1, 512), (1372, 2048), (2048, 1)),
        ((1372, 512), (512, 1), (512, 2048), (2048, 1)),
        
        # 0.27%
        ((168, 2048), (2048, 1), (2048, 128256), (1, 2048)),
        ((128256, 168), (1, 128256), (168, 2048), (2048, 1)),
        ((168, 128256), (128256, 1), (128256, 2048), (2048, 1)),
        
        # 0.24%
        ((512, 1160), (1, 512), (1160, 2048), (2048, 1)),
        ((1160, 512), (512, 1), (512, 2048), (2048, 1)),
        
        # 0.20%
        ((124, 2048), (2048, 1), (2048, 128256), (1, 2048)),
        ((128256, 124), (1, 128256), (124, 2048), (2048, 1)),
        ((124, 128256), (128256, 1), (128256, 2048), (2048, 1)),
        
        # 0.13%
        ((512, 648), (1, 512), (648, 2048), (2048, 1)),
        ((648, 512), (512, 1), (512, 2048), (2048, 1)),
        ((512, 628), (1, 512), (628, 2048), (2048, 1)),
        ((628, 512), (512, 1), (512, 2048), (2048, 1)),
        
        # 0.11%
        ((68, 2048), (2048, 1), (2048, 128256), (1, 2048)),
        ((128256, 68), (1, 128256), (68, 2048), (2048, 1)),
        ((68, 128256), (128256, 1), (128256, 2048), (2048, 1)),
        
        # 0.10%
        ((512, 484), (1, 512), (484, 2048), (2048, 1)),
        ((484, 512), (512, 1), (512, 2048), (2048, 1)),
        ((60, 2048), (2048, 1), (2048, 128256), (1, 2048)),
        ((128256, 60), (1, 128256), (60, 2048), (2048, 1)),
        ((60, 128256), (128256, 1), (128256, 2048), (2048, 1)),
        
        # 0.06%
        ((36, 2048), (2048, 1), (2048, 128256), (1, 2048)),
        ((128256, 36), (1, 128256), (36, 2048), (2048, 1)),
        ((36, 128256), (128256, 1), (128256, 2048), (2048, 1))
    ]

    results = []
    # Process each shape tuple directly
    for shape1, stride1, shape2, stride2 in shapes:
        mat1 = create_tensor_with_strides(shape1, stride1, dtype, device)
        mat2 = create_tensor_with_strides(shape2, stride2, dtype, device)
        try:
            result = torch.mm(mat1, mat2)
            results.append(result)
            print(f"Successfully computed mm for shapes {shape1} x {shape2} -> {result.shape}")
        except RuntimeError as e:
            print(f"Failed for shapes {shape1} x {shape2}: {e}")

    return results

if __name__ == "__main__":
    run_mm_operations() 
